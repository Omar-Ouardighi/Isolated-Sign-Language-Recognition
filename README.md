# Isolated-Sign-Language-Recognition

This project aims to develop an isolated sign language recognition model that can identify signs made in landmark data extracted from raw videos of sign language gestures. The model will be trained using a combination of LSTM and GRU network and evaluated based on its accuracy in identifying the correct sign gestures.

# Results
My model achieved an accuracy of 70.5% on the test set. While this accuracy is moderate, there is room for improvement to achieve higher recognition rates.
The sparse top-5 categorical accuracy was measured at 88.16%. This metric considers the correct gesture prediction being within the top 5 predicted gestures. This indicates that the model has a reasonable understanding of the gestures and can often identify the correct gesture among the top predictions.

# Future work
By considering the complexity of sign language gestures, augmenting the training data, exploring diverse datasets, optimizing hyperparameters, and evaluating the models using multiple metrics, we can refine the model architecture and improve the performance for sign language gesture recognition.
